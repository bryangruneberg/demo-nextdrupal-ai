{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "- This is a Python Jupyter notebook but the framework, concepts, and tooling translatest to JS/TS easily\n",
    "- The live Chatbot demo was built with NextJS/React and Typescript\n",
    "- This is very code heavy so bear with me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/justinwinter/.pyenv/versions/3.11.4/lib/python3.11/site-packages (0.0.257)\n",
      "Requirement already satisfied: faiss-cpu in /Users/justinwinter/.pyenv/versions/3.11.4/lib/python3.11/site-packages (1.7.4)\n",
      "Requirement already satisfied: openai in /Users/justinwinter/.pyenv/versions/3.11.4/lib/python3.11/site-packages (0.27.8)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/justinwinter/.pyenv/versions/3.11.4/lib/python3.11/site-packages (4.12.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/justinwinter/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/justinwinter/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from langchain) (2.0.17)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/justinwinter/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /Users/justinwinter/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from langchain) (0.5.8)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.11 in /Users/justinwinter/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from langchain) (0.0.14)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /Users/justinwinter/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from langchain) (2.8.4)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/justinwinter/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from langchain) (1.24.4)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /Users/justinwinter/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /Users/justinwinter/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from langchain) (1.10.9)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/justinwinter/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/justinwinter/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: tqdm in /Users/justinwinter/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/justinwinter/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from beautifulsoup4) (2.4.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/justinwinter/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/justinwinter/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/justinwinter/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/justinwinter/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/justinwinter/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/justinwinter/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/justinwinter/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /Users/justinwinter/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /Users/justinwinter/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /Users/justinwinter/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/justinwinter/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from pydantic<2,>=1->langchain) (4.7.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/justinwinter/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/justinwinter/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from requests<3,>=2->langchain) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/justinwinter/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/justinwinter/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/justinwinter/.pyenv/versions/3.11.4/lib/python3.11/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain faiss-cpu openai beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG) - Basic Example\n",
    "\n",
    "### 1. Creating vector datastores, text splitting, and document creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 81 documents\n",
      "You have 81 embeddings\n",
      "Here's a sample of one: [0.012702980829312127, 0.011727926082949343, 0.01778281026350076]...\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Load the document - this is just a transcript from a recent podcast\n",
    "# Invest Like the Best with Patrick O'Shaughnessy, Ep Des Traynor - Real Talk about AI and Software - Aug 8, 2023\n",
    "# https://podcasts.apple.com/us/podcast/des-traynor-real-talk-about-ai-and-software/id1154105909?i=1000623777607\n",
    "raw_documents = TextLoader('./data/paul-graham-great-work.txt').load()\n",
    "\n",
    "# Split the document into chunks\n",
    "documents = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50).split_documents(raw_documents)\n",
    "print (f\"You have {len(documents)} documents\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "embedding_list = embeddings.embed_documents([text.page_content for text in documents])\n",
    "print (f\"You have {len(embedding_list)} embeddings\")\n",
    "print (f\"Here's a sample of one: {embedding_list[0][:3]}...\")\n",
    "\n",
    "db = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Query our vector datastore directly for relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We now have 3 relevant docs\n",
      "\n",
      "Here is the first doc:\n",
      "(Document(page_content=\"Great work usually entails spending what would seem to most people an unreasonable amount of time on a problem. You can't think of this time as a cost, or it will seem too high. You have to find the work sufficiently engaging as it's happening.\\n\\nThere may be some jobs where you have to work diligently for years at things you hate before you get to the good part, but this is not how great work happens. Great work happens by focusing consistently on something you're genuinely interested in. When you pause to take stock, you're surprised how far you've come.\\n\\nThe reason we're surprised is that we underestimate the cumulative effect of work. Writing a page a day doesn't sound like much, but if you do it every day you'll write a book a year. That's the key: consistency. People who do great things don't get a lot done every day. They get something done, rather than nothing.\", metadata={'source': './data/paul-graham-great-work.txt'}), 0.8176611008116834)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the key to doing great work?\"\n",
    "relevant_docs = db.similarity_search_with_relevance_scores(query, 3)\n",
    "\n",
    "print(f\"We now have {len(relevant_docs)} relevant docs\")\n",
    "print()\n",
    "\n",
    "# print the first three docs with a space in between\n",
    "print(\"Here is the first doc:\")\n",
    "for doc in relevant_docs[:1]:\n",
    "    print(doc)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Let's \"stuff\" these documents into a context so we can query against them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "# openai_api_base='https://llama2-api.main.llama2-api.test1.amazee.io/v1'\n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(openai_api_base='https://llama2-api.main.llama2-api.test1.amazee.io/v1'), chain_type=\"stuff\", retriever=db.as_retriever(), chain_type_kwargs=chain_type_kwargs)\n",
    "\n",
    "\n",
    "result = qa.run(\"What are three things anyone can do to achieve great work? Please list them in order of importance as a numbered list.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### 1) Identify and focus on an area of interest that you have a natural aptitude for and a deep interest in; 2) Maintain curiosity, trying new things, meeting people, reading books, and asking questions to increase your chances of encountering opportunities; 3) Consistently work on this area of interest, even if only making progress at a small rate each day."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "formatted_text = '\\n'.join([f'### {line}' for line in result.strip().split('\\n')])\n",
    "\n",
    "display(Markdown(formatted_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##\n",
    "\n",
    "## <[ BACK TO SLIDES ]>\n",
    "\n",
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Umami Food Blog Chatbot Implementation (Complex Example)\n",
    "Building on the above steps\n",
    "\n",
    "1. Setup\n",
    "2. Data prep/mapping\n",
    "3. Creating custom documents\n",
    "4. Leveraging LLM to create custom Tags & add them back to our documents\n",
    "5. Create a Vectorstore from our documents\n",
    "6. Introducing \"memory\" to chatbot context for chat_history\n",
    "7. Create our \"chain\" and querying the chatbot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup: Import the necessary libraries, initialize the LLM, and setup the prompt\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv('./.env')\n",
    "\n",
    "# Get OPENAI_API_KEY and BEARER_TOKEN from .env file\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# initialize LLM (we use ChatOpenAI because we'll later define a `chat` agent)\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    temperature=0,\n",
    "    model_name='gpt-3.5-turbo',\n",
    ")\n",
    "\n",
    "# Setup the prompt to generate the tags based on the content and specify the format we're looking for\n",
    "prompt = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "You are a content tagging bot for a food and drink blog, and your role is to identify and tag recipes. \n",
    "Specify whether the item is a food or drink and then focus on the type (e.g., vegan, gluten-free), unique ingredients (e.g., dried fruit, super seeds), cooking or preparation techniques (e.g., grilling, soaking, mixing), dietary restrictions, cultural origins, meal or occasion types (e.g., breakfast, lunch, dinner, cocktail party), and special flavors or features (e.g., sweet, savory, spicy) that stand out in the given recipe.\n",
    "\n",
    "Recipe: {recipe}\n",
    "\n",
    "Feel free to add any tags that may provide insightful information about the dish or drink. There's no maximum number of tags, so be thorough and descriptive in your tagging, as it helps readers find recipes that match their preferences.\n",
    "YOU MUST return the tags in the following format:\n",
    "\n",
    "Category: Drink, Type: Vegan, Unique Ingredients: Mint leaves, Preparation Techniques: Mixing, Dietary Restrictions: Gluten-free, Cultural Origins: Cuban, Occasion: Cocktail party, Special Features: Refreshing]\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# put the prompt together with the LLM we want to use\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2 & 3. Data Prep/Mapping, Creating Custom Documents - Load the JSON API export, clean the HTML, and create a Document object for each page\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from bs4 import BeautifulSoup\n",
    "import asyncio\n",
    "import json\n",
    "\n",
    "# Load our data expore - could be a URL or a local file\n",
    "with open('./ingest/composetheweb.json', 'r') as file:\n",
    "    jsonapi_export = json.load(file)\n",
    "\n",
    "\n",
    "# Clean the HTML from the JSON API export\n",
    "def clean_html(html_content):\n",
    "    return BeautifulSoup(html_content, 'html.parser').get_text()\n",
    "\n",
    "# Create and format the document object\n",
    "def create_document(doc):\n",
    "    title = doc['attributes']['title']\n",
    "    source = doc['attributes']['path']['alias']\n",
    "    metadata = {'title': title, 'source': source}\n",
    "    \n",
    "    if doc['type'] == 'node--recipe':\n",
    "        difficulty = str(doc['attributes']['field_difficulty'])\n",
    "        ingredients = str(doc['attributes']['field_ingredients'])\n",
    "        recipe = clean_html(str(doc['attributes']['field_recipe_instruction']['value']))\n",
    "        summary = clean_html(str(doc['attributes']['field_summary']['value']))\n",
    "        \n",
    "        page_content = f\"Title: {title},\\nDifficulty: {difficulty},\\n\\nIngredients: {ingredients},\\n\\nRecipe: {recipe}, \\n\\nSummary: {summary}\"\n",
    "    else:\n",
    "        body = clean_html(str(doc['attributes']['body']['value']))\n",
    "        page_content = f\"Title: {title} - {body}\"\n",
    "    \n",
    "    return Document(metadata=metadata, page_content=page_content)\n",
    "\n",
    "# loop over the JSON API export and create a Document object for each page\n",
    "docs = [create_document(doc) for doc in jsonapi_export['data']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO NOT RUN DURING DEMO - ~ 1-2 min to run\n",
    "\n",
    "# 4. Leveraging LLM to create custom Tags & add them back to our documents - Why? See Side Note below...\n",
    "\n",
    "# Simple wrapper function around the chain invocation\n",
    "async def generateTagsFromContent(content):\n",
    "    return chain.invoke({\"recipe\": content})\n",
    "\n",
    "# Run the documents back through a LLM to generate tags\n",
    "generateTagPromises = [generateTagsFromContent(doc.page_content) for doc in docs]\n",
    "tags = await asyncio.gather(*generateTagPromises)\n",
    "\n",
    "# loop over the tags and add them to the metadata\n",
    "updatedDocs = []\n",
    "for index, doc in enumerate(docs):\n",
    "    updatedDocs.append(\n",
    "        Document(\n",
    "            metadata={**doc.metadata, 'tags': tags[index]['text']}, \n",
    "            page_content=doc.page_content\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector store...\n"
     ]
    }
   ],
   "source": [
    "# 5. Create a Vectorstore from our documents\n",
    "\n",
    "print(\"Creating vector store...\")\n",
    "vectorstore: FAISS = FAISS.from_documents(updatedDocs, OpenAIEmbeddings())\n",
    "vectorstore.save_local(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 18\n",
      "\n",
      "Example Document Content: (First 300 chars)\n",
      "Title: Borscht with pork ribs,\n",
      "Difficulty: medium,\n",
      "\n",
      "Ingredients: ['400-500g pork ribs', '2 beets', '2 tomatoes', '¼ celery root', '¼ cabbage', '3-4 medium potatoes', '1 carrot', '1 onion', '1-2 smoked pears', '2 bay leaves', '3 allspice berries', '1 bulb garlic', '1 bell pepper', '200ml tomato juice\n",
      "------------------\n",
      "Metadata object containing our source and generated tags\n",
      "{'title': 'Borscht with pork ribs', 'source': '/recipes/borscht-with-pork-ribs', 'tags': 'Category: Food, Type: Non-vegetarian, Unique Ingredients: Pork ribs, Smoked pears, Preparation Techniques: Baking, Simmering, Sautéing, Stewing, Dietary Restrictions: None, Cultural Origins: Ukrainian, Occasion: Lunch, Dinner, Special Features: Hearty, Flavorful, Savory]'}\n"
     ]
    }
   ],
   "source": [
    "# Recap - Let's take a peek at the data in our vectorstore\n",
    "print(f\"Number of documents: {len(updatedDocs)}\")\n",
    "print()\n",
    "print(\"Example Document Content: (First 300 chars)\")\n",
    "print(updatedDocs[8].page_content[:300])\n",
    "print('------------------')\n",
    "print(\"Metadata object containing our source and generated tags\")\n",
    "print(updatedDocs[8].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Borscht with pork ribs,\n",
      "Difficulty: medium,\n",
      "\n",
      "Ingredients: ['400-500g pork ribs', '2 beets', '...\n",
      "Metadata: {'title': 'Borscht with pork ribs', 'source': '/recipes/borscht-with-pork-ribs', 'tags': 'Category: Food, Type: Non-vegetarian, Unique Ingredients: Pork ribs, Smoked pears, Preparation Techniques: Baking, Simmering, Sautéing, Stewing, Dietary Restrictions: None, Cultural Origins: Ukrainian, Occasion: Lunch, Dinner, Special Features: Hearty, Flavorful, Savory]'}\n",
      "Relavance Score: 0.6947289853895937\n"
     ]
    }
   ],
   "source": [
    "# Side Note: Why have an LLM create tags?\n",
    "# Generated tags allow us to create a much more useful and broad search of our content\n",
    "\n",
    "# Example - Ukrainian Dishes are not mentioned directly in the recipe at all. \n",
    "# Searching for this via a traditional Drupal or Solr search would not be possible without manually adding tags like this\n",
    "question = \"What Ukranian dishes do we have?\"\n",
    "\n",
    "found_docs = vectorstore.similarity_search_with_relevance_scores(question )\n",
    "print(f\"{found_docs[0][0].page_content[:100]}...\")\n",
    "print(f\"Metadata: {found_docs[0][0].metadata}\")\n",
    "print(f\"Relavance Score: {found_docs[0][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains & Memory\n",
    "\n",
    "What is a chain?\n",
    "\n",
    "\"Chain\" - Langchain Framework Construct: Just the ability to combine multiple LLMs in series and/or with logic applied\n",
    "This allows us to implement much deeper systems and workflows.\n",
    "\n",
    "Example: Combine Vector Search + Question, Follow up question w/ original Vector Search + Conversation history, Etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Advanced Chain)  ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Introducing \"memory\" to chatbot context for chat_history\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "question_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)\n",
    "\n",
    "# note we get the sources w/ this chain\n",
    "doc_chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Borscht is a traditional Ukrainian dish made with pork ribs, beets, tomatoes, celery root, cabbage, potatoes, carrot, onion, smoked pears, bay leaves, allspice berries, garlic, bell pepper, tomato juice, butter, tomato paste, water, sour cream, and beans (optional).\n",
      "SOURCES: /recipes/borscht-with-pork-ribs...\n"
     ]
    }
   ],
   "source": [
    "# 7. Create our \"chain\" and querying the chatbot\n",
    "\n",
    "chain = ConversationalRetrievalChain(\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    question_generator=question_generator,\n",
    "    combine_docs_chain=doc_chain,\n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "chat_history = []\n",
    "query = \"Do we have any Ukranian dishes? Please respond with just the title and a single sentence description.\"\n",
    "result = chain({\"question\": query})\n",
    "\n",
    "print(f\"{result['answer'][:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The full recipe for Borscht with pork ribs is:\n",
      "400-500g pork ribs, 2 beets, 2 tomatoes, ¼ celery root, ¼ cabbage, 3-4 medium potatoes, 1 carrot, 1 onion, 1-2 smoked pears, 2 bay leaves, 3 allspice berries, 1 bulb garlic, 1 bell pepper, 200ml tomato juice, 30g butter, 2 tbsp tomato paste, 3l water, Sour cream, 1 can beans (optional), Salt to taste.\n",
      "\n",
      "Preheat the oven to 400°F/200°C. Put pork ribs on the baking dish and bake them for 30 minutes. Wash but don’t peel the celery root. Dice the carrot. Put the baked pork ribs in a pot and add 3 liters of water. Chop the celery root and carrot and add them to the pot, along with half the unpeeled onion. Bring the pot to the boil and simmer for 30 minutes. The heart of the borscht is sautéed vegetables. Dice the bell pepper, tomatoes, and remainder of the onion. Place the butter in a saucepan and add the vegetables. Cook them over a moderate heat until soft. Add the tomato paste, reduce the\n"
     ]
    }
   ],
   "source": [
    "query = \"Can you provide me with the full recipe?\"\n",
    "result = chain(query)\n",
    "\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ HumanMessage(content='Do we have any Ukranian dishes? Please respond with just the title and a single sentence description.', additional_kwargs={}, example=False),\n",
      "  AIMessage(content=' Title: Borscht with pork ribs, Description: A traditional Ukrainian dish made with pork ribs, beets, tomatoes, celery root, cabbage, potatoes, carrot, onion, smoked pears, bay leaves, allspice berries, garlic, bell pepper, tomato juice, butter, tomato paste, and water.\\nSOURCES: /recipes/borscht-with-pork-ribs', additional_kwargs={}, example=False),\n",
      "  HumanMessage(content='Do we have any Ukranian dishes? Please respond with just the title and a single sentence description.', additional_kwargs={}, example=False),\n",
      "  AIMessage(content=' Borscht is a traditional Ukrainian dish made with pork ribs, beets, tomatoes, celery root, cabbage, potatoes, carrot, onion, smoked pears, bay leaves, allspice berries, garlic, bell pepper, tomato juice, butter, tomato paste, water, sour cream, and beans (optional).\\nSOURCES: /recipes/borscht-with-pork-ribs', additional_kwargs={}, example=False),\n",
      "  HumanMessage(content='Can you provide me with the full recipe?', additional_kwargs={}, example=False),\n",
      "  AIMessage(content=' The full recipe for Borscht with pork ribs is:\\n400-500g pork ribs, 2 beets, 2 tomatoes, ¼ celery root, ¼ cabbage, 3-4 medium potatoes, 1 carrot, 1 onion, 1-2 smoked pears, 2 bay leaves, 3 allspice berries, 1 bulb garlic, 1 bell pepper, 200ml tomato juice, 30g butter, 2 tbsp tomato paste, 3l water, Sour cream, 1 can beans (optional), Salt to taste.\\n\\nPreheat the oven to 400°F/200°C. Put pork ribs on the baking dish and bake them for 30 minutes. Wash but don’t peel the celery root. Dice the carrot. Put the baked pork ribs in a pot and add 3 liters of water. Chop the celery root and carrot and add them to the pot, along with half the unpeeled onion. Bring the pot to the boil and simmer for 30 minutes. The heart of the borscht is sautéed vegetables. Dice the bell pepper, tomatoes, and remainder of the onion. Place the butter in a saucepan and add the vegetables. Cook them over a moderate heat until soft. Add the tomato paste, reduce the', additional_kwargs={}, example=False)]\n"
     ]
    }
   ],
   "source": [
    "# pretty print the chat history object\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(result['chat_history'], indent=2, width=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Mapping, Optional\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM\n",
    "import requests\n",
    "from typing import Any, List, Mapping, Optional\n",
    "from langchain.llms.base import LLM\n",
    "\n",
    "class RemoteLLM(LLM):\n",
    "    url: str\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"remote\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "    ) -> str:\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "\n",
    "\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        body = {\n",
    "            \"model\": \"gpt-3.5-turbo\",\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_tokens\": 2000\n",
    "        }\n",
    "        \n",
    "        response = requests.post(self.url, json=body, headers=headers)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            raise Exception(\"Request failed with status code:\", response.status_code)\n",
    "        \n",
    "        response_content = response.json().get('choices', [{}])[0].get('message', {}).get('content', '')\n",
    "        return response_content\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {\"url\": self.url}\n",
    "\n",
    "\n",
    "llm = RemoteLLM(url=\"https://llama2-api.main.llama2-api.test1.amazee.io/v1/chat/completions\")\n",
    "response = llm(\"What's the biggest animal in the world?\")\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decoupled",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
